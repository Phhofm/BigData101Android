<resources>
    <string name="app_name">BigData101</string>

    <!-- TODO: Remove or change this placeholder text -->
    <string name="hello_blank_fragment">Hello blank fragment</string>

    <string name="element_text">Element</string>
    <string name="grid_layout_manager">Grid Layout Manager</string>
    <string name="linear_layout_manager">Linear Layout Manager</string>


    <string name="intro_text">Welcome to Big Data 101. Here you will learn concerning the technological as well as the legal aspects of Big Data. We will also have a look at the new GDPR and how this will affect companies in switzerland, espacially the insurance companies. You will also learn in this part, why the GDPR is sometimes loosely formulated and why there are no simple guidelines and rules a company can implement in regareds to the GDPR, since sometimes the circumstances are way more complicated than one might think. Giving the GDPR such a strong force with high penalties if not kept can also be catastrophical.</string>
    <string name="technology_text">Lets first have a look at the definition of Big Data. Big Data simply describes a data set that is too large to be processes by commercial end-user components like laptops or mobile phones or home computers. This data might be generated by sensors which input a hughe amount of data continuously. Or it might also be that the database of your insurance company might have grown too big for just a home computer to process. Just look at the data that a service like facebook gathers and processes. And so forth. Some text here.</string>
    <string name="law_text">Lets have a closer look here at the law aspect of this whole topic. We will look first at the GDPR, what it means in regards to big data nad in regards to insurcance companies in switzerland. we will then also look if there are some speicifc swiss laws that also touch on ths subject.</string>
    <string name="combination_text">Now that we know about both fields, lets try to bring them both together, and look at espaeially the difficulties that come with it. Like we learned, it is not that easy sto simpley geographically sort the members. Blabla. The GDPR wants to bring the world data security like the USA wants to bring the world democracy. IP-blocking? But I can simply spoof IP-Addressses. etc.</string>
    <string name="introduction">Introduction</string>
    <string name="technology">Technology</string>
    <string name="law">Law</string>
    <string name="combination">Combination</string>
    <string name="big_data_101">Big Data 101</string>
    <string name="legal_disclaimer">No advice\nThis app contains general information about legal matters. The information is not advice, and should not be treated as such.\nLimitation of warranties\nThe legal information on this app is provided “as is” without any representations or warranties, express or implied. BigData101 makes no representations or warranties in relation to the legal information on this app. Without prejudice to the generality of the foregoing paragraph, BigData101 does not warrant that \nthe legal information on this app will be constantly available, or available at all; or\nthe legal information on this app is complete, true, accurate, up-to-date, or non-misleading.\nProfessional assistance\nYou must not rely on the information on this app as an alternative to legal advice from your attorney or other professional legal services provider. If you have any specific questions about any legal matter you should consult your attorney or other professional legal services provider. You should never delay seeking legal advice, disregard legal advice, or commence or discontinue any legal action because of information on this app.\nLiability\nNothing in this legal disclaimer will limit any of our liabilities in any way that is not permitted under applicable law, or exclude any of our liabilities that may not be excluded under applicable law.</string>
    <string name="accountability">accountability</string>
    <string name="data_protection_impact_assessment">data protection impact assessment</string>
    <string name="data_protection_officer">data protection officer</string>
    <string name="examples">examples</string>
    <string name="fines">fines</string>
    <string name="gdpr_principles">gdpr principles</string>
    <string name="how_and_where_data_is_stored">A system is needed that not only has the capacity to store this huge amount of data, but also provides fast access times and parallel access in case the data is used in different processes simultaneously. Since a single server that provides these attributes would be very expensive, commodity clusters have been the go to. A computer cluster is a set of connected computers that appear as a single system. Each computer, called node, runs its own instance of operating system and performs tasks, controlled and scheduled by software.Clusters are usually deployed to improve performance and availability over that of a single computer. Commodity custer simply means that the data center consists of interconnected consumer-grade hardware,because they are relatively cheap and available in large quantities. Those commodity clusters are much more cost-effective than single computers of comparable speed or availability. Since these consumer-grade hardware fail from time to time, we need to make sure to store multiple copies of a single file on different nodes (replication) in case a node fails, no data is lost. Also replication helps with the parallel access, since two different nodes can be accessed to get the same data.
We call this a parallel file system, which is a type of clustered file system, that spread data across multiple storage nodes for redundancy and performance. Those file systems have different design goals, they aim for access transparency (clients are unaware that files are distributed and can access them in the same way as local files), location transparency, replication transparency (client should be unaware of replicated files across multiple servers) and migration transparency (files move around without clients knowledge).
One of the solutions, the Hadoop Distributed File System (HDFS) splits data into blocks and stores them redundantly on the nodes. It has NameNodes, that stores metadata like which nodes are alives and is the primary node that gets accesses by a client that wants to access or store data. As a simplified example, if the client wants to store a file, it is split up into blocks. The client then asks the NameNode where to store the first block, and the NameNode answers with three node-addresses. The client then stores the first block on these three nodes, and repeats the whole process for the second block and then all following, until the whole file is stored. Accessing a file works in a similar manner.
</string>
    <string name="how_big_data_is_obtained">how big data is obtained</string>
    <string name="how_data_is_organized">how data is organized</string>
    <string name="how_dat_is_processed">The general workflow is simple. Data gets collected from an environment for example through sensors. This data is then processed to gain information, and this information is then analyzed to gain information. Information in this sense is created when information is interpreted and connected with other information.
Processing big data on distributed systems adds new complexities that did not exist in traditional systems. One needs to deal with consistency issues, data partitioning, load balancing and massively parallel algorithms. This requires new programming models to be able to reliably solve these challenges.
In general, there are different methods or strategies. The used methods are classification, where we predict to what income group a person belongs; class probability estimation, with which probability a person belongs to a certain income group; regression, how much income a specific person has; segmentation, where we cluster all people; and association rule mining, where we find functional dependencies. A cluster analysis finds groups of objects such that the object in the group will be similar to one another and different from objects in other groups. Simply said we find similarities and differences and build groups accordingly to specific criterias. Association rule mining finds rules that will predict the occurence of an item based on the occurences of other items.
For this processing, we use programming abstractions like MapReduce. It distributed the tasks and data automatically in a cluster and is platform agnostic. It was optimized for a hardware environment where CPUs and disk space is cheap and plentiful and Memory was limited. Since memory is becoming cheaper, Apache Spark responded to this trend by offering distributed in-memory processing. An example for a simple task could be a distributed word count in a very large text file, where data is split into chunks and countings run on it, and the separate results are in the end merged together.
</string>
    <string name="main_principles">main principles</string>
    <string name="privacy_by_design_and_default">privacy by design and default</string>
    <string name="problems_with_the_gdpr">problems with the gdpr</string>
    <string name="processor_and_controller">processor and controller</string>
    <string name="reprimands">reprimands</string>
    <string name="right_to_data_portability">right to data portability</string>
    <string name="right_to_erasure">right to erasure</string>
    <string name="security">security</string>
    <string name="terms_and_definitions">terms and definitions</string>
    <string name="theory"><li>hahah</li></string>
    <string name="what_is_big_data">Big data describes very large collections of data. These datasets are generally too large for conventional home computers or traditional applications to process it in a timely manner. The three properties of big data are Volume, Velocity and Variety. Today\'s organizations collect a lot of data (volume), from different sources. This data might involve all financial transactions, data collected through their website, all their customer data, data collected from their social media page (like comments), phone call metadata (timestamps and numbers), estimated values of buildings, history of buildings like when it was renovated, data collected by sensors and so forth. Simply the storing of such huge amount of collected data introduced new challenges. The data that gets collected through sensors generates a constant data-streams which must sometimes be structured or even processed in real time (velocity). This collection of data from very different sources comes in a variety of forms, mostly in an unstructured form or it might be already structured, in numerical form, or in plaintext, or as audio -file or -stream, and so forth. Structuring and/or processing of such very different formats is a complex challenge in itself.</string>
    <string name="what_is_big_data_used_for">Only the collecting and storing of huge amounts of data itself does not create a lot of value. The value is created by analyzing and processing this data. Data Science methods and processes allow us to extract knowledge from that data, to help us make an informed decision about specific matters. It allows us to find answers that enable us to for example generate coupons at the point of sale based on the customer’s buying habits (cumulus, super card). Other forms are to recalculate entire risk portfolios in minutes or to detect fraudulent behavior before it affects our organization.
Big Data can support the company in assessing and managing risks. The insurance industry is competitive and companies need to be profitable. Data science can help those companies in the development of their business model and insurance products and develop new tools for the company to understand and analyze their customer data, to help retaining customers and attracting prospective customers. More specifically, this data can be used to optimized and support marketing programs by optimizing the best return for investments, targeting the optimal potential customers for a specified market segment. We can also use area specific market analytics to understand the risks of introducing a new particular insurance product rather than only using predictive analysis.
</string>


</resources>
